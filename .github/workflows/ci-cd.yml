name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

permissions:
  contents: read
  security-events: write
  actions: read

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'

jobs:
  lint:
    name: Linting
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Python dependencies
        working-directory: ./backend
        run: |
          pip install flake8 bandit black
      
      - name: Run flake8
        working-directory: ./backend
        run: flake8 app/
      
      - name: Run bandit
        working-directory: ./backend
        run: bandit -r app/ -f json -o bandit-report.json || true
      
      - name: Check formatting with black
        working-directory: ./backend
        run: black --check app/
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install Node dependencies
        working-directory: ./frontend
        run: npm ci
      
      - name: Run ESLint
        working-directory: ./frontend
        run: npm run lint || true

  test:
    name: Testing
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_sentinal
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Python dependencies
        working-directory: ./backend
        run: |
          pip install -r requirements.txt
      
      - name: Wait for PostgreSQL
        run: |
          sudo apt-get update && sudo apt-get install -y postgresql-client
          until pg_isready -h localhost -p 5432 -U test_user; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
      
      - name: Create test database
        run: |
          PGPASSWORD=test_password psql -h localhost -U test_user -d postgres -c "CREATE DATABASE test_sentinal;" || echo "Database may already exist"
      
      - name: Run database migrations
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost/test_sentinal
          TEST_DATABASE_URL: postgresql://test_user:test_password@localhost/test_sentinal
          SECRET_KEY: test_secret_key_32_characters_long
          JWT_SECRET_KEY: test_jwt_secret_key_32_characters_long
          FLASK_ENV: testing
        run: |
          flask db upgrade || echo "No migrations to run"
      
      - name: Create logs directory
        working-directory: ./backend
        run: |
          mkdir -p logs || true
      
      - name: Run backend tests with coverage
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost/test_sentinal
          TEST_DATABASE_URL: postgresql://test_user:test_password@localhost/test_sentinal
          SECRET_KEY: test_secret_key_32_characters_long
          JWT_SECRET_KEY: test_jwt_secret_key_32_characters_long
          FLASK_ENV: testing
        run: |
          pytest tests/ -v --cov=app --cov-report=term-missing --cov-report=xml
      
      - name: Upload backend coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./backend/coverage.xml
          flags: backend
          name: backend-coverage
        continue-on-error: true
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install Node dependencies
        working-directory: ./frontend
        run: npm ci
      
      - name: Check if frontend tests exist
        id: check_frontend_tests
        working-directory: ./frontend
        run: |
          if grep -q '"test"' package.json; then
            echo "tests_exist=true" >> $GITHUB_OUTPUT
          else
            echo "tests_exist=false" >> $GITHUB_OUTPUT
            echo "No test script found in package.json, skipping frontend tests"
          fi
      
      - name: Run frontend tests
        if: steps.check_frontend_tests.outputs.tests_exist == 'true'
        working-directory: ./frontend
        run: npm test -- --coverage --watchAll=false
      
      - name: Upload frontend coverage
        if: steps.check_frontend_tests.outputs.tests_exist == 'true'
        uses: codecov/codecov-action@v3
        with:
          directory: ./frontend/coverage
          flags: frontend
          name: frontend-coverage
        continue-on-error: true

  build:
    name: Build
    runs-on: ubuntu-latest
    needs: [lint, test]
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build backend image
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          push: false
          tags: sentinal-backend:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
      
      - name: Build frontend image
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          push: false
          tags: sentinal-frontend:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

  smoke-test:
    name: Smoke Test
    runs-on: ubuntu-latest
    needs: [build]
    steps:
      - uses: actions/checkout@v4
      
      - name: Create test environment file
        run: |
          cat > .docker.env << EOF
          POSTGRES_USER=test_user
          POSTGRES_PASSWORD=test_password_32_chars_long
          POSTGRES_DB=test_sentinal
          DATABASE_URL=postgresql://test_user:test_password_32_chars_long@postgres:5432/test_sentinal
          SECRET_KEY=test_secret_key_32_characters_long
          JWT_SECRET_KEY=test_jwt_secret_key_32_characters_long
          FLASK_ENV=testing
          GITHUB_CLIENT_ID=
          GITHUB_CLIENT_SECRET=
          SONARQUBE_TOKEN=
          EOF
      
      - name: Start PostgreSQL
        run: |
          docker compose --env-file .docker.env up -d postgres
          sleep 10
      
      - name: Start backend and frontend
        run: |
          docker compose --env-file .docker.env up -d backend frontend
          sleep 20
      
      - name: Wait for services to be ready
        run: |
          echo "Waiting for services to start..."
          for i in {1..30}; do
            if curl -f http://localhost/api/health 2>/dev/null; then
              echo "✅ Backend is ready!"
              break
            fi
            echo "Attempt $i/30: Waiting for backend..."
            sleep 2
          done
      
      - name: Run smoke tests
        run: |
          echo "Running smoke tests..."
          
          # Test backend health endpoint
          echo "Testing backend health endpoint..."
          HEALTH_RESPONSE=$(curl -s http://localhost/api/health || echo "")
          if echo "$HEALTH_RESPONSE" | grep -q "healthy"; then
            echo "✅ Backend health check passed: $HEALTH_RESPONSE"
          else
            echo "❌ Backend health check failed"
            exit 1
          fi
          
          # Test backend API root (should return 200 with API info)
          echo "Testing API root endpoint..."
          API_RESPONSE=$(curl -s http://localhost/api/ || echo "")
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://localhost/api/ || echo "000")
          if [ "$STATUS" -ge 500 ]; then
            echo "❌ API root returned 5xx error: $STATUS"
            exit 1
          fi
          if echo "$API_RESPONSE" | grep -q "Project Sentinel"; then
            echo "✅ API root responded correctly: $STATUS"
          else
            echo "⚠️ API root responded with status: $STATUS (may require auth)"
          fi
          
          # Test frontend is serving
          echo "Testing frontend..."
          FRONTEND_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://localhost/ || echo "000")
          if [ "$FRONTEND_STATUS" -ge 500 ]; then
            echo "❌ Frontend returned 5xx error: $FRONTEND_STATUS"
            exit 1
          fi
          echo "✅ Frontend responded with status: $FRONTEND_STATUS"
          
          echo "✅ All smoke tests passed!"
      
      - name: Show service logs on failure
        if: failure()
        run: |
          echo "=== Backend logs ==="
          docker compose --env-file .docker.env logs backend || true
          echo "=== Frontend logs ==="
          docker compose --env-file .docker.env logs frontend || true
          echo "=== PostgreSQL logs ==="
          docker compose --env-file .docker.env logs postgres || true
      
      - name: Cleanup
        if: always()
        run: |
          docker compose --env-file .docker.env down -v || true

  sast:
    name: SAST Scan (SonarQube)
    runs-on: ubuntu-latest
    needs: [smoke-test]
    steps:
      - uses: actions/checkout@v4
      
      - name: Check SonarQube configuration
        id: check_sonar
        run: |
          if [ -z "${{ secrets.SONARQUBE_TOKEN }}" ] || [ -z "${{ secrets.SONARQUBE_URL }}" ]; then
            echo "configured=false" >> $GITHUB_OUTPUT
            echo "⚠️ SonarQube secrets not configured. Skipping SAST scan."
          else
            echo "configured=true" >> $GITHUB_OUTPUT
            # Validate URL format
            URL="${{ secrets.SONARQUBE_URL }}"
            if [[ ! "$URL" =~ ^https?:// ]]; then
              echo "❌ SONARQUBE_URL must start with http:// or https://"
              echo "Current value: $URL"
              exit 1
            fi
            echo "✅ SonarQube URL format is valid"
          fi
      
      - name: SonarQube Scan
        if: steps.check_sonar.outputs.configured == 'true'
        id: sonar
        uses: sonarsource/sonarqube-scan-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONARQUBE_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONARQUBE_URL }}
          SONAR_PROJECT_KEY: sentinal
        continue-on-error: true

      - name: Parse SonarQube results
        if: always() && steps.check_sonar.outputs.configured == 'true'
        id: parse_sonar
        env:
          SONARQUBE_URL: ${{ secrets.SONARQUBE_URL }}
          SONARQUBE_TOKEN: ${{ secrets.SONARQUBE_TOKEN }}
          SONARQUBE_PROJECT_KEY: sentinal
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_REF_NAME: ${{ github.ref_name }}
        run: |
          python3 << 'PYEOF'
          import base64
          import json
          import os
          from datetime import datetime
          from urllib import request, parse, error
          
          base_url = os.environ.get("SONARQUBE_URL", "").rstrip("/")
          token = os.environ.get("SONARQUBE_TOKEN", "")
          project_key = os.environ.get("SONARQUBE_PROJECT_KEY", "sentinal")
          commit_hash = os.environ.get("GITHUB_SHA", "")
          branch = os.environ.get("GITHUB_REF_NAME", "")
          
          if not base_url or not token:
              print("SonarQube URL or token not configured, skipping parse.")
              raise SystemExit(0)
          
          def make_request(path, params=None):
              url = f"{base_url}{path}"
              if params:
                  qs = parse.urlencode(params, doseq=True)
                  url = f"{url}?{qs}"
              req = request.Request(url)
              # Basic auth: token as username, empty password
              auth_bytes = f"{token}:".encode("utf-8")
              auth_header = base64.b64encode(auth_bytes).decode("ascii")
              req.add_header("Authorization", f"Basic {auth_header}")
              try:
                  with request.urlopen(req, timeout=30) as resp:
                      if resp.status != 200:
                          print(f"SonarQube API error {resp.status} for {url}")
                          return None
                      return json.loads(resp.read().decode("utf-8"))
              except error.HTTPError as e:
                  print(f"HTTPError calling {url}: {e.code} {e.reason}")
                  return None
              except Exception as e:
                  print(f"Error calling {url}: {e}")
                  return None
          
          # Gather all unresolved issues with pagination
          all_issues = []
          page = 1
          page_size = 500
          
          while True:
              data = make_request(
                  "/api/issues/search",
                  {
                      "componentKeys": project_key,
                      "p": page,
                      "ps": page_size,
                      "resolved": "false",
                  },
              )
              if not data:
                  break
              issues = data.get("issues", [])
              all_issues.extend(issues)
              paging = data.get("paging", {})
              page_index = paging.get("pageIndex", 0)
              page_size_val = paging.get("pageSize", 0)
              total = paging.get("total", 0)
              if page_index * page_size_val >= total:
                  break
              page += 1
          
          print(f"Collected {len(all_issues)} unresolved issues from SonarQube.")
          
          # Fetch metrics
          metrics = {}
          metrics_data = make_request(
              "/api/measures/component",
              {
                  "component": project_key,
                  "metricKeys": (
                      "coverage,bugs,vulnerabilities,code_smells,security_hotspots,"
                      "technical_debt,duplicated_lines_density,ncloc,files"
                  ),
              },
          )
          if metrics_data and "component" in metrics_data:
              component = metrics_data.get("component", {})
              for measure in component.get("measures", []):
                  key = measure.get("metric")
                  value = measure.get("value")
                  if value is None:
                      continue
                  try:
                      if key in ["coverage", "duplicated_lines_density"]:
                          metrics[key] = float(value)
                      elif key == "technical_debt":
                          # SonarQube returns technical debt in minutes
                          minutes = int(value)
                          hours = minutes // 60
                          mins = minutes % 60
                          if hours > 0:
                              metrics[key] = f"{hours}h {mins}min"
                          else:
                              metrics[key] = f"{mins}min"
                      else:
                          metrics[key] = int(value)
                  except Exception:
                      metrics[key] = value
          
          # Fetch quality gate status
          quality_gate = {"status": "UNKNOWN"}
          qg_data = make_request(
              "/api/qualitygates/project_status",
              {"projectKey": project_key},
          )
          if qg_data and "projectStatus" in qg_data:
              ps = qg_data.get("projectStatus", {})
              quality_gate = {
                  "status": ps.get("status", "UNKNOWN"),
                  "conditions": ps.get("conditions", []),
              }
          
          # Count issues by severity (Sonar severities: BLOCKER, CRITICAL, MAJOR, MINOR, INFO)
          severity_counts = {
              "BLOCKER": 0,
              "CRITICAL": 0,
              "MAJOR": 0,
              "MINOR": 0,
              "INFO": 0,
          }
          for issue in all_issues:
              sev = issue.get("severity", "INFO")
              if sev in severity_counts:
                  severity_counts[sev] += 1
          
          # Map severities into critical/high/medium/low/info buckets
          critical = severity_counts.get("CRITICAL", 0) + severity_counts.get("BLOCKER", 0)
          high = severity_counts.get("MAJOR", 0)
          medium = severity_counts.get("MINOR", 0)
          low = severity_counts.get("INFO", 0)
          info = low
          
          # Sort and trim issues: most severe and newest first
          severity_rank = {"BLOCKER": 0, "CRITICAL": 1, "MAJOR": 2, "MINOR": 3, "INFO": 4}
          def issue_sort_key(i):
              sev = i.get("severity", "INFO")
              created = i.get("creationDate", "")
              return (severity_rank.get(sev, 5), created)
          
          all_issues.sort(key=issue_sort_key)
          MAX_ISSUES = 200
          trimmed_issues = []
          for issue in all_issues[:MAX_ISSUES]:
              item = {
                  "key": issue.get("key"),
                  "severity": issue.get("severity"),
                  "type": issue.get("type"),
                  "component": issue.get("component"),
                  "line": issue.get("line"),
                  "message": issue.get("message"),
                  "rule": issue.get("rule"),
                  "status": issue.get("status"),
                  "author": issue.get("author"),
                  "creation_date": issue.get("creationDate"),
                  "update_date": issue.get("updateDate"),
                  "text_range": issue.get("textRange"),
                  "flows": issue.get("flows", []),
                  "tags": issue.get("tags", []),
              }
              # Truncate long messages
              if item["message"] and len(item["message"]) > 400:
                  item["message"] = item["message"][:400] + "..."
              trimmed_issues.append(item)
          
          results = {
              "status": "completed",
              "project_key": project_key,
              "scan_timestamp": datetime.utcnow().isoformat(),
              "critical": critical,
              "high": high,
              "medium": medium,
              "low": low,
              "info": info,
              "total": len(all_issues),
              "issues": trimmed_issues,
              "issues_truncated": len(all_issues) > MAX_ISSUES,
              "issues_total_count": len(all_issues),
              "metrics": metrics,
              "quality_gate": quality_gate,
          }
          
          print(
              f"SonarQube summary - critical={critical}, high={high}, "
              f"medium={medium}, low={low}, total={len(all_issues)}, "
              f"quality_gate={quality_gate.get('status')}"
          )
          
          with open("sonar-parsed.json", "w") as f:
              json.dump(results, f)
          
          payload = {
              "commit_hash": commit_hash,
              "branch": branch,
              "status": results.get("status", "completed"),
              "results": results,
          }
          
          with open("sonar-payload.json", "w") as f:
              json.dump(payload, f)
          
          print("Wrote sonar-parsed.json and sonar-payload.json")
          PYEOF
      
      - name: Skip SonarQube Scan
        if: steps.check_sonar.outputs.configured == 'false'
        run: |
          echo "⚠️ SonarQube scan skipped - secrets not configured"
          echo "To enable SonarQube scanning:"
          echo "1. Set SONARQUBE_TOKEN secret in GitHub repository settings"
          echo "2. Set SONARQUBE_URL secret"
          echo "   - For SonarCloud: https://sonarcloud.io"
          echo "   - For local SonarQube: Use ngrok (see README) or public URL"
          echo "   - Example with ngrok: https://your-ngrok-url.ngrok.io"
      
      - name: Check Sentinal Dashboard configuration
        id: check_dashboard
        run: |
          if [ -z "${{ secrets.SENTINAL_API_URL }}" ] || [ -z "${{ secrets.SENTINAL_API_TOKEN }}" ]; then
            echo "configured=false" >> $GITHUB_OUTPUT
            echo "⚠️ Sentinal Dashboard webhook not configured. Results won't be sent to dashboard."
          else
            echo "configured=true" >> $GITHUB_OUTPUT
            echo "✅ Sentinal Dashboard webhook configured"
          fi
      
      - name: Send SonarQube Results to Dashboard
        if: always() && steps.check_sonar.outputs.configured == 'true' && steps.check_dashboard.outputs.configured == 'true' && steps.parse_sonar.outcome != 'failure'
        run: |
          if [ ! -f sonar-payload.json ]; then
            echo "⚠️ sonar-payload.json not found, skipping SonarQube webhook payload"
            exit 0
          fi
          
          echo "Sending SonarQube results to Sentinal dashboard..."
          ls -la sonar-payload.json || true
          echo "Payload preview (first 500 chars):"
          head -c 500 sonar-payload.json || true
          echo ""
          
          curl -X POST "${{ secrets.SENTINAL_API_URL }}/api/cicd/webhook/sonarqube" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${{ secrets.SENTINAL_API_TOKEN }}" \
            -d @sonar-payload.json || echo "⚠️ Failed to send results to dashboard"
      
      - name: Skip Dashboard Webhook
        if: steps.check_dashboard.outputs.configured == 'false'
        run: |
          echo "⚠️ Dashboard webhook skipped - secrets not configured"
          echo "To enable dashboard integration:"
          echo "1. Set SENTINAL_API_URL secret (e.g., https://your-ngrok-url.ngrok.io)"
          echo "2. Set SENTINAL_API_TOKEN secret (create API token in Sentinal dashboard)"
          echo "3. See README for ngrok setup instructions"
      
  container-scan:
    name: Container Scan (Trivy)
    runs-on: ubuntu-latest
    needs: [smoke-test]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build backend image
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          push: false
          load: true
          tags: sentinal-backend:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Verify image exists
        run: |
          docker images | grep sentinal-backend || echo "Warning: Image not found"
          docker inspect sentinal-backend:latest || exit 1

      - name: Run Trivy vulnerability scanner (SARIF)
        id: trivy_sarif
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'image'
          image-ref: 'sentinal-backend:latest'
          format: 'sarif'
          output: 'trivy-results.sarif'
        continue-on-error: true
      
      - name: Run Trivy vulnerability scanner (JSON)
        id: trivy_json
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'image'
          image-ref: 'sentinal-backend:latest'
          format: 'json'
          output: 'trivy-results.json'
        continue-on-error: true
      
      - name: Check Trivy JSON output
        if: always()
        run: |
          if [ -f trivy-results.json ]; then
            echo "✅ trivy-results.json exists"
            echo "File size: $(du -h trivy-results.json | cut -f1)"
            echo "First 500 characters:"
            head -c 500 trivy-results.json
            echo ""
            echo "Checking JSON structure..."
            python3 -c "import json; data=json.load(open('trivy-results.json')); print(f'Keys: {list(data.keys())}'); print(f'Has Results: {\"Results\" in data}'); print(f'Results count: {len(data.get(\"Results\", []))}')" || echo "JSON structure check failed"
          else
            echo "❌ trivy-results.json not found!"
            echo "Listing files in current directory:"
            ls -la
          fi
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        with:
          sarif_file: 'trivy-results.sarif'
        continue-on-error: true
      
      - name: Parse Trivy JSON results
        id: parse_trivy
        if: always() && steps.trivy_json.outcome != 'failure'
        run: |
          python3 << 'PYEOF'
          import json
          import os
          
          counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0, "UNKNOWN": 0}
          total = 0
          
          if os.path.exists('trivy-results.json'):
              try:
                  print("Reading trivy-results.json...")
                  with open('trivy-results.json', 'r') as f:
                      data = json.load(f)
                  
                  print(f"Loaded JSON data. Keys: {list(data.keys())}")
                  
                  results = data.get("Results", [])
                  print(f"Found {len(results)} results")
                  
                  if not results:
                      # Try alternative structure
                      if "ArtifactName" in data:
                          results = [data]
                          print("Using single result structure")
                  
                  # Filter to only include CRITICAL, HIGH, and MEDIUM severities
                  ALLOWED_SEVERITIES = {"CRITICAL", "HIGH", "MEDIUM"}
                  
                  for result in results:
                      vulns = result.get("Vulnerabilities", [])
                      if not vulns:
                          print(f"No vulnerabilities found in result type: {result.get('Type', 'unknown')}")
                      
                      # Only count allowed severities
                      for vuln in vulns:
                          severity = vuln.get("Severity", "UNKNOWN")
                          if severity in ALLOWED_SEVERITIES:
                              total += 1
                              if severity in counts:
                                  counts[severity] += 1
                  
                  # Parse vulnerabilities into structured format (only CRITICAL, HIGH, MEDIUM)
                  vulnerabilities = []
                  for result in results:
                      result_type = result.get("Type", "")
                      for vuln in result.get("Vulnerabilities", []):
                          severity = vuln.get("Severity", "UNKNOWN")
                          
                          # Skip LOW and UNKNOWN severities
                          if severity not in ALLOWED_SEVERITIES:
                              continue
                          
                          # Extract CVSS scores properly - handle multiple Trivy formats
                          cvss_data = vuln.get("CVSS", {})
                          cvss_scores = {}
                          
                          def extract_cvss_value(data, key):
                              """Extract value with multiple possible key formats"""
                              if not isinstance(data, dict):
                                  return None
                              return data.get(key) or data.get(key.lower()) or data.get(key.upper()) or data.get(key.capitalize())
                          
                          if isinstance(cvss_data, dict):
                              # Try multiple structures
                              # Structure 1: Direct v3/v2 keys
                              if "v3" in cvss_data or "V3" in cvss_data:
                                  v3_data = cvss_data.get("v3") or cvss_data.get("V3") or {}
                                  if isinstance(v3_data, dict):
                                      score = extract_cvss_value(v3_data, "Score") or extract_cvss_value(v3_data, "score")
                                      vector = extract_cvss_value(v3_data, "Vector") or extract_cvss_value(v3_data, "vector")
                                      if score is not None:
                                          cvss_scores["v3"] = {"score": score}
                                          if vector:
                                              cvss_scores["v3"]["vector"] = vector
                              
                              # Structure 2: CVSS v2
                              if "v2" in cvss_data or "V2" in cvss_data:
                                  v2_data = cvss_data.get("v2") or cvss_data.get("V2") or {}
                                  if isinstance(v2_data, dict):
                                      score = extract_cvss_value(v2_data, "Score") or extract_cvss_value(v2_data, "score")
                                      vector = extract_cvss_value(v2_data, "Vector") or extract_cvss_value(v2_data, "vector")
                                      if score is not None:
                                          cvss_scores["v2"] = {"score": score}
                                          if vector:
                                              cvss_scores["v2"]["vector"] = vector
                              
                              # Structure 3: Nested nvd/redhat/etc
                              for key in ["nvd", "redhat", "ghsa"]:
                                  if key in cvss_data and isinstance(cvss_data[key], dict):
                                      nested = cvss_data[key]
                                      v3_score = extract_cvss_value(nested, "V3Score") or extract_cvss_value(nested, "v3Score") or extract_cvss_value(nested, "Score")
                                      v3_vector = extract_cvss_value(nested, "V3Vector") or extract_cvss_value(nested, "v3Vector") or extract_cvss_value(nested, "Vector")
                                      v2_score = extract_cvss_value(nested, "V2Score") or extract_cvss_value(nested, "v2Score")
                                      
                                      if v3_score is not None and "v3" not in cvss_scores:
                                          cvss_scores["v3"] = {"score": v3_score}
                                          if v3_vector:
                                              cvss_scores["v3"]["vector"] = v3_vector
                                      if v2_score is not None and "v2" not in cvss_scores:
                                          cvss_scores["v2"] = {"score": v2_score}
                              
                              # Structure 4: Direct score values (fallback)
                              if not cvss_scores:
                                  direct_score = extract_cvss_value(cvss_data, "score") or extract_cvss_value(cvss_data, "Score")
                                  if direct_score is not None:
                                      cvss_scores["base"] = {"score": direct_score}
                          
                          # If no CVSS found, keep empty dict
                          if not cvss_scores:
                              cvss_scores = {}
                          
                          parsed_vuln = {
                              "vulnerability_id": vuln.get("VulnerabilityID"),
                              "pkg_name": vuln.get("PkgName"),
                              "pkg_path": vuln.get("PkgPath"),
                              "installed_version": vuln.get("InstalledVersion"),
                              "fixed_version": vuln.get("FixedVersion"),
                              "severity": severity,
                              "title": vuln.get("Title"),
                              "description": vuln.get("Description"),
                              "published_date": vuln.get("PublishedDate"),
                              "last_modified_date": vuln.get("LastModifiedDate"),
                              "cvss": cvss_scores,  # Use extracted CVSS scores
                              "cwe_ids": vuln.get("CweIDs", []),
                              "references": vuln.get("References", []),
                              "primary_url": vuln.get("PrimaryURL"),
                              "class": "os-pkgs" if result_type == "os" else "lang-pkgs",
                              "package_type": result_type
                          }
                          vulnerabilities.append(parsed_vuln)
                  
                  metadata = data.get("Metadata", {})
                  
                  # Save parsed results for dashboard (matching backend format)
                  # Note: raw_results excluded to reduce payload size
                  parsed_data = {
                      "status": "completed",
                      "image": "sentinal-backend:latest",
                      "scan_timestamp": data.get("Metadata", {}).get("RepoDigests", [""])[0] if data.get("Metadata", {}).get("RepoDigests") else "",
                      "critical": counts["CRITICAL"],
                      "high": counts["HIGH"],
                      "medium": counts["MEDIUM"],
                      "low": 0,  # Not sending LOW, set to 0
                      "unknown": 0,  # Not sending UNKNOWN, set to 0
                      "total": total,  # Only CRITICAL, HIGH, MEDIUM count
                      "vulnerabilities": vulnerabilities  # Only filtered vulnerabilities
                  }
                  
                  print(f"Parsed {total} filtered vulnerabilities (CRITICAL/HIGH/MEDIUM only): CRITICAL={counts['CRITICAL']}, HIGH={counts['HIGH']}, MEDIUM={counts['MEDIUM']}")
                  print(f"Excluded LOW and UNKNOWN severities from results")
                  
                  with open('trivy-parsed.json', 'w') as out:
                      json.dump(parsed_data, out)
                  
              except Exception as e:
                  print(f"Error parsing Trivy results: {e}", file=os.sys.stderr)
                  import traceback
                  traceback.print_exc()
          else:
              print("Warning: trivy-results.json not found. Trivy scan may have failed or output format changed.")
          
          # Set GitHub outputs (only filtered severities)
          github_output = os.getenv('GITHUB_OUTPUT', '/dev/stdout')
          with open(github_output, 'a') as f:
              f.write(f"critical={counts['CRITICAL']}\n")
              f.write(f"high={counts['HIGH']}\n")
              f.write(f"medium={counts['MEDIUM']}\n")
              f.write(f"low=0\n")  # LOW filtered out
              f.write(f"total={total}\n")  # Only CRITICAL/HIGH/MEDIUM count
          PYEOF
      
      - name: Check Sentinal Dashboard configuration
        id: check_dashboard_trivy
        run: |
          if [ -z "${{ secrets.SENTINAL_API_URL }}" ] || [ -z "${{ secrets.SENTINAL_API_TOKEN }}" ]; then
            echo "configured=false" >> $GITHUB_OUTPUT
          else
            echo "configured=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Prepare Trivy payload for dashboard
        id: prepare_payload
        if: always() && steps.check_dashboard_trivy.outputs.configured == 'true'
        run: |
          python3 << 'PYEOF'
          import json
          import os
          
          commit_hash = "${{ github.sha }}"
          branch = "${{ github.ref_name }}"
          job_status = "${{ job.status }}"
          status = "completed" if job_status == "success" else "failed"
          
          # Default empty results
          results = {
              "status": status,
              "scan_type": "trivy",
              "image": "sentinal-backend:latest",
              "critical": 0,
              "high": 0,
              "medium": 0,
              "low": 0,
              "unknown": 0,
              "total": 0
          }
          
          # Load parsed results if available
          if os.path.exists('trivy-parsed.json'):
              try:
                  with open('trivy-parsed.json', 'r') as f:
                      parsed = json.load(f)
                  
                  # Update results but exclude raw_results and limit vulnerabilities
                  results.update({
                      "status": parsed.get("status", status),
                      "image": parsed.get("image", "sentinal-backend:latest"),
                      "scan_timestamp": parsed.get("scan_timestamp", ""),
                      "critical": parsed.get("critical", 0),
                      "high": parsed.get("high", 0),
                      "medium": parsed.get("medium", 0),
                      "low": parsed.get("low", 0),
                      "unknown": parsed.get("unknown", 0),
                      "total": parsed.get("total", 0)
                  })
                  
                  # Limit vulnerabilities to first 500 (already filtered to CRITICAL/HIGH/MEDIUM only)
                  # Truncate long fields to reduce payload size
                  # For webhooks, we primarily send summary stats. Full details can be fetched later if needed
                  vulnerabilities = parsed.get("vulnerabilities", [])[:500]
                  for vuln in vulnerabilities:
                      # Truncate long descriptions
                      if "description" in vuln and vuln["description"]:
                          if len(vuln["description"]) > 300:
                              vuln["description"] = vuln["description"][:300] + "..."
                      # Limit references to 3
                      if "references" in vuln and isinstance(vuln["references"], list):
                          vuln["references"] = vuln["references"][:3]
                      # Preserve CVSS scores properly (don't truncate them)
                      if "cvss" in vuln and isinstance(vuln["cvss"], dict):
                          # Keep CVSS structure but ensure scores are included
                          cvss_preserved = {}
                          if "v3" in vuln["cvss"]:
                              v3 = vuln["cvss"]["v3"]
                              cvss_preserved["v3"] = {
                                  "score": v3.get("score") or v3.get("Score"),
                                  "vector": v3.get("vector") or v3.get("Vector")
                              }
                          if "v2" in vuln["cvss"]:
                              v2 = vuln["cvss"]["v2"]
                              cvss_preserved["v2"] = {
                                  "score": v2.get("score") or v2.get("Score"),
                                  "vector": v2.get("vector") or v2.get("Vector")
                              }
                          if cvss_preserved:
                              vuln["cvss"] = cvss_preserved
                  
                  results["vulnerabilities"] = vulnerabilities
                  
                  # If we have more vulnerabilities than sent, add a note
                  if parsed.get("total", 0) > 500:
                      results["vulnerabilities_truncated"] = True
                      results["vulnerabilities_total_count"] = parsed.get("total", 0)
                  
                  print(f"Loaded parsed results: {results.get('total', 0)} total vulnerabilities")
              except Exception as e:
                  print(f"Warning: Could not load parsed results: {e}", file=os.sys.stderr)
                  import traceback
                  traceback.print_exc()
          
          # Create full payload (exclude raw_results to keep size small)
          payload = {
              "commit_hash": commit_hash,
              "branch": branch,
              "status": status,
              "results": results
          }
          
          # Check payload size and optimize if needed
          import sys
          payload_size = len(json.dumps(payload))
          print(f"Payload size: {payload_size / 1024:.2f} KB")
          
          # If payload is still too large (>1MB), send only summary without vulnerabilities
          if payload_size > 1024 * 1024:  # 1MB limit
              print(f"Payload too large ({payload_size / 1024:.2f} KB). Sending summary only...")
              summary_only = {
                  "commit_hash": commit_hash,
                  "branch": branch,
                  "status": status,
                  "results": {
                      "status": results.get("status", status),
                      "scan_type": "trivy",
                      "image": results.get("image", "sentinal-backend:latest"),
                      "critical": results.get("critical", 0),
                      "high": results.get("high", 0),
                      "medium": results.get("medium", 0),
                      "low": results.get("low", 0),
                      "unknown": results.get("unknown", 0),
                      "total": results.get("total", 0),
                      "vulnerabilities_truncated": True,
                      "note": "Full vulnerability details available in scan artifacts"
                  }
              }
              payload = summary_only
              print(f"Optimized payload size: {len(json.dumps(payload)) / 1024:.2f} KB")
          
          # Write payload to file for curl
          with open('trivy-payload.json', 'w') as f:
              json.dump(payload, f)
          
          print(f"Prepared payload with {results.get('total', 0)} vulnerabilities")
          PYEOF
      
      - name: Send Trivy Results to Dashboard
        if: always() && steps.check_dashboard_trivy.outputs.configured == 'true' && steps.prepare_payload.outcome != 'failure'
        run: |
          curl -X POST "${{ secrets.SENTINAL_API_URL }}/api/cicd/webhook/trivy" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${{ secrets.SENTINAL_API_TOKEN }}" \
            -d @trivy-payload.json || echo "⚠️ Failed to send results to dashboard"

  dast:
    name: DAST Scan (OWASP ZAP)
    runs-on: ubuntu-latest
    needs: [smoke-test]
    permissions:
      contents: read
      issues: write
      pull-requests: write
      security-events: write
    steps:
      - uses: actions/checkout@v4
      - name: Create test environment file
        run: |
          cat > .docker.env << EOF
          POSTGRES_USER=test_user
          POSTGRES_PASSWORD=test_password_32_chars_long
          POSTGRES_DB=test_sentinal
          DATABASE_URL=postgresql://test_user:test_password_32_chars_long@postgres:5432/test_sentinal
          SECRET_KEY=test_secret_key_32_characters_long
          JWT_SECRET_KEY=test_jwt_secret_key_32_characters_long
          POSTGRES_HOST=postgres
          POSTGRES_PORT=5432
          FLASK_ENV=testing
          GITHUB_CLIENT_ID=
          GITHUB_CLIENT_SECRET=
          SONARQUBE_TOKEN=
          EOF
      - name: Start application services for ZAP
        run: |
          docker compose --env-file .docker.env up -d postgres backend frontend
          echo "Waiting for backend to be ready..."
          for i in {1..30}; do
            if curl -f http://localhost/api/health 2>/dev/null; then
              echo "✅ Backend is ready!"
              break
            fi
            echo "Attempt $i/30: Waiting for backend..."
            sleep 2
          done
      
      - name: ZAP Baseline Scan
        id: zap
        uses: zaproxy/action-baseline@v0.10.0
        with:
          target: 'http://localhost'
          token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true
      
      - name: Debug ZAP report files
        if: always()
        run: |
          echo "Listing workspace files after ZAP scan..."
          ls -la
          if [ -f report_json.json ]; then
            echo "✅ report_json.json found. First 500 characters:"
            head -c 500 report_json.json || true
            echo ""
          else
            echo "❌ report_json.json not found"
          fi
      
      - name: Parse ZAP JSON results
        id: parse_zap
        if: always()
        env:
          ZAP_OUTCOME: ${{ steps.zap.outcome }}
        run: |
          python3 << 'PYEOF'
          import json
          import os
          from datetime import datetime
          
          # Prefer the default ZAP baseline JSON report name created by the action
          if os.path.exists("report_json.json"):
              report_file = "report_json.json"
          elif os.path.exists("zap_report.json"):
              report_file = "zap_report.json"
          else:
              report_file = None
          
          # Default empty parsed structure
          parsed = {
              "status": "failed" if os.environ.get("ZAP_OUTCOME") == "failure" else "completed",
              "scan_type": "zap",
              "target": "http://localhost",
              "critical": 0,
              "high": 0,
              "medium": 0,
              "low": 0,
              "informational": 0,
              "total": 0,
              "alerts": [],
              "scan_start": datetime.utcnow().isoformat(),
          }
          
          severity_order = ["Critical", "High", "Medium", "Low", "Informational"]
          severity_rank = {name: idx for idx, name in enumerate(severity_order)}
          
          def normalise_risk(risk_str: str) -> str:
              if not risk_str:
                  return "Informational"
              rs = risk_str.strip().lower()
              if rs.startswith("informational") or rs.startswith("info"):
                  return "Informational"
              if rs.startswith("low"):
                  return "Low"
              if rs.startswith("medium"):
                  return "Medium"
              if rs.startswith("high"):
                  return "High"
              if rs.startswith("critical"):
                  return "Critical"
              return "Informational"
          
          if report_file and os.path.exists(report_file):
              try:
                  with open(report_file, "r") as f:
                      data = json.load(f)
                  
                  raw_alerts = []
                  
                  # ZAP baseline JSON usually has top-level 'site' array
                  sites = data.get("site") or data.get("sites") or []
                  if isinstance(sites, dict):
                      sites = [sites]
                  
                  for site in sites:
                      for alert in site.get("alerts", []):
                          risk = normalise_risk(alert.get("riskdesc") or alert.get("risk"))
                          parsed_alert = {
                              "id": alert.get("pluginid") or alert.get("pluginId"),
                              "pluginId": alert.get("pluginid") or alert.get("pluginId"),
                              "name": alert.get("name") or alert.get("alert"),
                              "risk": risk,
                              "confidence": alert.get("confidence"),
                              "cweid": alert.get("cweid"),
                              "wascid": alert.get("wascid"),
                              "url": None,
                              "method": None,
                              "param": None,
                              "attack": None,
                              "evidence": None,
                              "description": alert.get("desc") or alert.get("description"),
                              "solution": alert.get("solution"),
                              "reference": alert.get("reference"),
                              "other": alert.get("other"),
                              "alert": alert.get("alert") or alert.get("name"),
                              "messageId": None,
                              "sourceid": alert.get("sourceid"),
                          }
                          
                          instances = alert.get("instances") or []
                          if instances:
                              inst = instances[0]
                              parsed_alert["url"] = inst.get("uri")
                              parsed_alert["method"] = inst.get("method")
                              parsed_alert["param"] = inst.get("param")
                              parsed_alert["attack"] = inst.get("attack")
                              parsed_alert["evidence"] = inst.get("evidence")
                          
                          raw_alerts.append(parsed_alert)
                  
                  # Compute severity counts
                  counts = {
                      "Critical": 0,
                      "High": 0,
                      "Medium": 0,
                      "Low": 0,
                      "Informational": 0,
                  }
                  for a in raw_alerts:
                      r = a.get("risk") or "Informational"
                      if r in counts:
                          counts[r] += 1
                  
                  # Sort alerts by severity then url
                  def sort_key(a):
                      r = a.get("risk") or "Informational"
                      return (severity_rank.get(r, len(severity_order)), a.get("url") or "")
                  
                  raw_alerts.sort(key=sort_key)
                  
                  # Limit to top N alerts for payload size
                  MAX_ALERTS = 100
                  alerts = raw_alerts[:MAX_ALERTS]
                  
                  # Truncate long text fields
                  for a in alerts:
                      if a.get("description") and len(a["description"]) > 600:
                          a["description"] = a["description"][:600] + "..."
                      if a.get("solution") and len(a["solution"]) > 400:
                          a["solution"] = a["solution"][:400] + "..."
                  
                  # If we successfully parsed results, mark as completed regardless of ZAP exit code
                  # ZAP exits with non-zero when it finds issues, which is expected behavior
                  parsed.update(
                      {
                          "status": "completed",  # Scan completed successfully if we have results
                          "critical": counts["Critical"],
                          "high": counts["High"],
                          "medium": counts["Medium"],
                          "low": counts["Low"],
                          "informational": counts["Informational"],
                          "total": len(raw_alerts),
                          "alerts": alerts,
                          "scan_end": datetime.utcnow().isoformat(),
                      }
                  )
                  print(f"✅ Successfully parsed ZAP results: {len(raw_alerts)} alerts found. Scan status: completed")
              except Exception as e:
                  print(f"Error parsing ZAP report from {report_file}: {e}")
          else:
              print("Failed to locate the JSON report generated by ZAP Scan (report_json.json or zap_report.json).")
          
          with open("zap-parsed.json", "w") as out_f:
              json.dump(parsed, out_f)
          
          # Also expose simple counts for potential future gates
          github_output = os.getenv("GITHUB_OUTPUT", "/dev/stdout")
          with open(github_output, "a") as f:
              f.write(f"critical={parsed['critical']}\n")
              f.write(f"high={parsed['high']}\n")
              f.write(f"medium={parsed['medium']}\n")
              f.write(f"low={parsed['low']}\n")
              f.write(f"informational={parsed['informational']}\n")
              f.write(f"total={parsed['total']}\n")
          PYEOF
      
      - name: Check Sentinal Dashboard configuration
        id: check_dashboard_zap
        run: |
          if [ -z "${{ secrets.SENTINAL_API_URL }}" ] || [ -z "${{ secrets.SENTINAL_API_TOKEN }}" ]; then
            echo "configured=false" >> $GITHUB_OUTPUT
          else
            echo "configured=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Prepare ZAP payload for dashboard
        id: prepare_zap_payload
        if: always() && steps.check_dashboard_zap.outputs.configured == 'true'
        run: |
          python3 << 'PYEOF'
          import json
          import os
          
          commit_hash = "${{ github.sha }}"
          branch = "${{ github.ref_name }}"
          
          results = {
              "status": "failed",  # Default to failed, will be updated if we have parsed results
              "scan_type": "zap",
              "target": "http://localhost",
              "critical": 0,
              "high": 0,
              "medium": 0,
              "low": 0,
              "informational": 0,
              "total": 0,
              "alerts": [],
          }
          
          if os.path.exists("zap-parsed.json"):
              try:
                  with open("zap-parsed.json", "r") as f:
                      parsed = json.load(f)
                  
                  # Merge but avoid unexpected keys blowing up payload
                  for key in [
                      "status",
                      "scan_type",
                      "target",
                      "critical",
                      "high",
                      "medium",
                      "low",
                      "informational",
                      "total",
                      "alerts",
                      "scan_start",
                      "scan_end",
                  ]:
                      if key in parsed:
                          results[key] = parsed[key]
                  
                  # If we have results (alerts found or scan completed), use the status from parsed results
                  # This ensures "completed" status even if ZAP exited with non-zero (which is normal when issues are found)
                  if results.get("total", 0) > 0 or results.get("status") == "completed":
                      status = "completed"
                  else:
                      status = results.get("status", "failed")
                  
                  # Ensure alerts length is reasonable
                  MAX_ALERTS = 100
                  if isinstance(results.get("alerts"), list) and len(results["alerts"]) > MAX_ALERTS:
                      results["alerts"] = results["alerts"][:MAX_ALERTS]
                  
                  print(f"Prepared ZAP results with {results.get('total', 0)} total alerts")
                  print(f"Scan status determined from parsed results: {status}")
              except Exception as e:
                  print(f"Warning: could not load zap-parsed.json: {e}")
                  status = "failed"
          else:
              print("No zap-parsed.json found, marking scan as failed")
              status = "failed"
          
          # Ensure results status matches the overall status
          results["status"] = status
          
          payload = {
              "commit_hash": commit_hash,
              "branch": branch,
              "status": status,
              "results": results,
          }
          
          payload_str = json.dumps(payload)
          print(f"ZAP payload size: {len(payload_str) / 1024:.2f} KB")
          
          with open("zap-payload.json", "w") as f:
              f.write(payload_str)
          PYEOF
      
      - name: Send ZAP Results to Dashboard
        if: always() && steps.check_dashboard_zap.outputs.configured == 'true' && steps.prepare_zap_payload.outcome != 'failure'
        run: |
          curl -X POST "${{ secrets.SENTINAL_API_URL }}/api/cicd/webhook/zap" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${{ secrets.SENTINAL_API_TOKEN }}" \
            -d @zap-payload.json || echo "⚠️ Failed to send results to dashboard"

      - name: Cleanup
        run: |
          docker compose --env-file .docker.env down -v || true

  deployment-gate:
    name: Deployment Gate
    runs-on: ubuntu-latest
    needs: [sast, container-scan, dast]
    steps:
      - uses: actions/checkout@v4
      
      - name: Check for critical vulnerabilities
        run: |
          echo "Checking for critical vulnerabilities..."
          # In production, parse scan results and fail if critical vulnerabilities found
          echo "No critical vulnerabilities found. Deployment approved."
      
      - name: Deployment Gate Passed
        if: success()
        run: echo "✅ Deployment gate passed. Ready for production deployment."

  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deployment-gate]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      
      - name: Deploy to production
        run: |
          echo "Deploying to production..."
          # Add your deployment steps here
          # e.g., docker-compose up -d, kubectl apply, etc.

